{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory Topics\n",
    "- Perceptron Model to Neural Networks\n",
    "- Activation Functions\n",
    "- Cost Functions\n",
    "- Feed Forward Networks\n",
    "- BackPropagation\n",
    "\n",
    "Coding Topics\n",
    "- Tensorflow 2.0 Keras Syntax\n",
    "- ANN with Keras\n",
    "    - Regression\n",
    "    - Classification\n",
    "- Exercises for Keras ANN\n",
    "- Tensorboard Visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Model\n",
    "\n",
    "To begin understanding deep learning, we will build up our model abstractions:\n",
    "- Single Biological Neuron\n",
    "- Perceptron\n",
    "- Multi-layer Perceptron Model\n",
    "- Deep Learning Neural Network\n",
    "\n",
    "As we learn about more complex models, we'll introduce concepts, such as:\n",
    "- Activation Functions\n",
    "- Gradient Descent \n",
    "- Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron was a form of neural network introduced in 1958 by Frank Rosenblatt.\n",
    "\n",
    "However, in 1969 Marvin Minsky and Seymour Papert's published their book Perceptrons.\n",
    "\n",
    "It suggested that there were severe limitations to what perceptrons could do.\n",
    "\n",
    "This marked the beginning of what is known as the AI winter, with little funding into AI and Neural Networks in the 1970s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "- A single perceptron will not be enough to learn complicated systems.\n",
    "- Fortunately, we can expand on the idea of a single perceptron, to create a multi-layer perceptron model.\n",
    "\n",
    "To build a network of perceptions, we can connect layers of perceptrons, using a multi-layer perceptron model.\n",
    "\n",
    "The outputs of one perceptions are directly fed into as inputs to another perceptron.\n",
    "\n",
    "This allows the network as a whole to learn about interactions and relationships between features.\n",
    "\n",
    "The first layer is the input layer.\n",
    "\n",
    "The last layer is the outer layer. This last layer can be more than one neuron.\n",
    "\n",
    "Layers in between the input and output layers are the hidden layers.\n",
    "\n",
    "Hidden layers are difficult to interpret, due to their high interconnectivity and distance away from known input and output.\n",
    "\n",
    "Neural networks become \"deep neural networks\" if then contains 2 or more hidden layers.\n",
    "\n",
    "In classification tasks, it would be useful to have all outputs fall between 0 and 1.\n",
    "These values can then present probability assignments for each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Inputs x have a weight w and a bias term b attached to them in the perceptron model.\n",
    "Which means we have:\n",
    "x * w + b\n",
    "\n",
    "Clearly w implies how much weight or strength to give the incoming input. We can think of b as an offset value, making x * w have to reach to a certain threshold before having an effect.\n",
    "\n",
    "For example  if b = -10\n",
    "- x * w + b\n",
    "\n",
    "Then the effects of x*w will not really start to overcome the bias until their product surpasses 10.\n",
    "After that, then the effect is solely based on the value of w. Thus the term \"bias\".\n",
    "\n",
    "Next we want to set boundaries for the overall output value of: x * w + b\n",
    "\n",
    "We can state: z = x * w + b\n",
    "\n",
    "And then pass z through some activation function to limit its value.\n",
    "\n",
    "The most simple networks rely on a basic step function that outputs 0 or 1. This sort of functions could be useful for classification ( 0 or 1 class). However this is a very \"strong\" function, since small changes are not reflected. There is a immediate cut off that splits between 0 and 1.\n",
    "\n",
    "Lucky for us, this is the sigmoid function! \n",
    " F(z)  = 1 / ( 1 + e ^ -z)\n",
    "\n",
    "Some activation functions:\n",
    "- Hyperbolic Tangent: tanh(z)\n",
    "- Rectified Linear Unit (ReLU): This is actually a relatively simple function: max (0, z) ReLu has been found to have very good performance, especially when dealing with the issue of vanishing gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class classification\n",
    "\n",
    "There are 2 main types of multi-class situations\n",
    "- Non - Exclusive Classes: A data point can have multiple classes/categories assigned to it.\n",
    "- Mutually Exclusive Classes: Only one class per data point.\n",
    "\n",
    "#### Non-Exclusive Classes\n",
    "- E.g. Photos can have multiple tags (e.g. beach, family, vacation, etc.)\n",
    "\n",
    "#### Mutually Exclusive Classes\n",
    "- Photos can be categorized as being in greyscale (black and white) or full color photos. A photo can not be both at the same time.\n",
    "\n",
    "#### Organizing Multiple Classes \n",
    "The easiest way to organize multiple classes is to simply have 1 output node per class.\n",
    "\n",
    "### Non-exclusive \n",
    "- Sigmoid Function:\n",
    "Each neuron will output a value between 0 and 1, indicating the probability of having that class assigned to it. This allows each neuron to output independent of the other classes, allowing for a single data point fed into the function to have multiple classes assigned to it.\n",
    "\n",
    "### Mutually Exclusive Classes\n",
    "- Softmax function: Softmax function calculates the probabilities distribution of the event over K different events. This function will calculate the probabilities of each target class over all possible target classes.\n",
    "\n",
    "The range will be 0 to 1, and the sum of all the proabilities will be equal to one. The model returns the probabailites of each class and the target class chosen will have the highest probability.\n",
    "\n",
    "\n",
    "#### Review\n",
    "- Perceptrons expanded to neural network model\n",
    "- Weight and Biases\n",
    "- Activation Functions \n",
    "- Time to learn about Cost Functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Functions and Gradient Descent\n",
    "\n",
    "We understand that neural networks take in inputs, multiply them by weights, and add biases to them. Then this result is passed through an activation function which at the end of all the layers leads to some output.\n",
    "\n",
    "The output y is the model estimation of what it ptrdicts the label to be. So after the network creates its prediction, how do we evaluate it?\n",
    "And after the evaluation how can we update the network's weights and biases?\n",
    "\n",
    "We need to take the estimated outputs of the network and then compare them to the real values of the label. Keep in mind this is using the training data set during the fitting/training of the model.\n",
    "\n",
    "The cost function (loss function) must be an average so it can output a single value. We can keep track of our loss/cost during training to monitor network performance.\n",
    "\n",
    "We will us the following variables:\n",
    "- y to represent the true value.\n",
    "- a to represent neuron's prediction.\n",
    "\n",
    "In terms of weights and bias:\n",
    "- w * x + b = z\n",
    "- Pass z into activation function f(z) = a\n",
    "\n",
    "One very common cost function is the quadratic cost function; We simply calculate the difference between the real values y(x) against our predicted values a(x).\n",
    "\n",
    "We can think of the cost function as: C(W,B, S, E ); W is our neural network's weights, B is our neural network's biases, S is the input of a single training sample, and E is the desired output of that training sample.\n",
    "\n",
    "This also means that if we have a huge network, we can expect C to be quite complex, with huge vectors of weights and biases.\n",
    "\n",
    "### Gradient Descent \n",
    "- We could start with larger steps, then go smaller as we realize the slope gets closer to zero. This is known as adaptive gradient.\n",
    "\n",
    "In 2015, Kingma and Ba published their paper: \"Adam: A Method for Stochastic Optimization\" Adam is a much more efficient way of searching for these minimums, so you will see us it for our code! Realisrically we're calculating this descent in an n-dimensional space for all our weights. When dealing with these N-dimensional vectors (tensors), the notation changes from derivative to gradient.\n",
    "\n",
    "From classification problems, we often use the cross entropy loss function. The assumption is that your model predicts a probability distribution p(y = i) for each class i = 1,2,....., C.\n",
    "\n",
    "\n",
    "Review:\n",
    "- Cost Fuctions\n",
    "- Gradient Descent \n",
    "- Adam Optimizer\n",
    "- Quadratic Coast and Cross - Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagations\n",
    "\n",
    "Fundamentally, we want to know how the cost function results changes with respect to the weights in the network, so we can update the weights to minimize the cost function.\n",
    "\n",
    "Each input will receive a weight and a bias. This mean we have: C(w1, b1, w2, b2, w3,b3). \n",
    "\n",
    "The main idea here is that we can use the gradient to go back through the network and adjust our weights and biases to minimize  the output of the error vector on the last output layer.\n",
    "\n",
    "Using some calculus notation, we can expand this idea to networks with multiple neurons per layer.\n",
    "\n",
    "Hadamard Products (Element by Element mutltiplication)\n",
    "\n",
    "### Learning Process of the Neural Network\n",
    "\n",
    "- Step 1: Using input x set the activation function a for the input layer.\n",
    "    - z = w * a + b\n",
    "    - a = f(Z)\n",
    "\n",
    "- This resulting a then feeds into the next layer (and so on).\n",
    "\n",
    "- Step 2: for each layer, compute:\n",
    "    - z(l) = w(l) * a (l-1) + b (l)\n",
    "    - a(l) = f(z(l))\n",
    "\n",
    "- Step 3: We compute our error vector:\n",
    "    - Expressing the rate of change of C with respect to the output activations.\n",
    "\n",
    "- Step 4: Backpropagate the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Keras and Tensorflow\n",
    "\n",
    "TensorFlow is an open-source deep learning library developed by Google, with TF 2.0 being officially realeased in late 2019.\n",
    "TensorFlow has a large ecosystem of related components, including libraries like Tensorboard, Deployment and Production APIs, and support for various programming languages.\n",
    "\n",
    "\n",
    "Keras is a high-level python library that can use a variety of deep learning libraries underneath, such as: TensorFlow, CNTK, or Theano.\n",
    "\n",
    "TensorFlow 1.x had a complex python class system for building models, and due to the huge popularity of Keras, when TF 2.0 was released, TF adopted Keras as the official API for TF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Syntax Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"USA_Housing.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n",
    "       'Avg. Area Number of Bedrooms', 'Area Population']].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Price\"].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(4, activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_dict = model.history.history  # This remains a dictionary\n",
    "loss_df = pd.DataFrame(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=X_test, y = y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=X_train, y=y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pd.Series(test_predictions.reshape(1500,))\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(y_test, columns=[\"Test True Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat([pred_df, test_predictions], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.columns= [\"Test True Y\", \"Model Predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Test True Y\", y = \"Model Predictions\", data= pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_a_e = mean_absolute_error(pred_df[\"Test True Y\"], pred_df[\"Model Predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_price = df[\"Price\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((m_a_e / mean_price) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(pred_df[\"Test True Y\"], pred_df[\"Model Predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_squared_error(pred_df[\"Test True Y\"], pred_df[\"Model Predictions\"]))** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in the Keras native format\n",
    "model.save('housing_prediction.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "later_model = load_model(\"housing_prediction.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "later_model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
